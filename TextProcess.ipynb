{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在做NLP的深度学习任务时，一个关键的问题是如何构建输入。本文介绍如何利用有限内存进行大规模数据处理，主要包括：\n",
    "    1. 建立词典\n",
    "    2. 将单词转换为id\n",
    "    3. 训练集验证集切分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法一\n",
    "\n",
    "\n",
    "参考： https://state-of-art.top/2018/11/28/%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import operator\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import config\n",
    "from Logginger import init_logger\n",
    "\n",
    "logger = init_logger(\"torch\", logging_path=config.LOG_PATH)\n",
    "\n",
    "\n",
    "def sent_label_split(line):\n",
    "    \"\"\"\n",
    "    句子处理成单词\n",
    "    :param line: 原始行\n",
    "    :return: 单词， 标签\n",
    "    \"\"\"\n",
    "    line = line.strip('\\n').split('@')\n",
    "    label = line[0]\n",
    "    sent = line[1].split(' ')\n",
    "    return sent, label\n",
    "\n",
    "def word_to_id(word, word2id):\n",
    "    \"\"\"\n",
    "    单词-->ID\n",
    "    :param word: 单词\n",
    "    :param word2id: word2id @type: dict\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return word2id[word] if word in word2id else word2id['unk']\n",
    "\n",
    "\n",
    "def bulid_vocab(vocab_size, min_freq=3, stop_word_list=None,\n",
    "                is_debug=False):\n",
    "    \"\"\"\n",
    "    建立词典\n",
    "    :param vocab_size: 词典大小\n",
    "    :param min_freq: 最小词频限制\n",
    "    :param stop_list: 停用词 @type：file_path\n",
    "    :param is_debug: 是否测试模式 @type: bool True:使用很小的数据集进行代码测试\n",
    "    :return: word2id\n",
    "    \"\"\"\n",
    "    size = 0\n",
    "    count = Counter()\n",
    "\n",
    "    with open(os.path.join(config.ROOT_DIR, config.RAW_DATA), 'r') as fr:\n",
    "        logger.info('Building vocab')\n",
    "        for line in tqdm(fr, desc='Build vocab'):\n",
    "            words, label = sent_label_split(line)\n",
    "            count.update(words)\n",
    "            size += 1\n",
    "            if is_debug:\n",
    "                limit_train_size = 10000\n",
    "                if size > limit_train_size:\n",
    "                    break\n",
    "    if stop_word_list:\n",
    "        stop_list = {}\n",
    "        with open(os.path.join(config.ROOT_DIR, config.STOP_WORD_LIST), 'r') as fr:\n",
    "                for i, line in enumerate(fr):\n",
    "                    word = line.strip('\\n')\n",
    "                    if stop_list.get(word) is None:\n",
    "                        stop_list[word] = i\n",
    "        count = {k: v for k, v in count.items() if k not in stop_list}\n",
    "    count = sorted(count.items(), key=operator.itemgetter(1))\n",
    "    # 词典\n",
    "    vocab = [w[0] for w in count if w[1] >= min_freq]\n",
    "    if vocab_size < len(vocab):\n",
    "        vocab = vocab[:vocab_size]\n",
    "    vocab = config.flag_words + vocab\n",
    "    logger.info('vocab_size is %d'%len(vocab))\n",
    "    # 词典到编号的映射\n",
    "    word2id = {k: v for k, v in zip(vocab, range(0, len(vocab)))}\n",
    "    assert word2id['<pad>'] == 0, \"ValueError: '<pad>' id is not 0\"\n",
    "    print(word2id)\n",
    "    with open(config.WORD2ID_FILE, 'wb') as fw:\n",
    "        pickle.dump(word2id, fw)\n",
    "    return word2id\n",
    "\n",
    "\n",
    "def train_val_split(X, y, valid_size=0.3, random_state=2018, shuffle=True):\n",
    "    \"\"\"\n",
    "    训练集验证集分割\n",
    "    :param X: sentences\n",
    "    :param y: labels\n",
    "    :param random_state: 随机种子\n",
    "    \"\"\"\n",
    "    logger.info('train val split')\n",
    "    data = [(data_x, data_y) for data_x, data_y in zip(X, y)]\n",
    "    N = len(data)\n",
    "    test_size = int(N * valid_size)\n",
    "\n",
    "    if shuffle:\n",
    "        random.seed(random_state)\n",
    "        random.shuffle(data)\n",
    "\n",
    "    valid = data[:test_size]\n",
    "    train = data[test_size:]\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "def text2id(word2id, maxlen=None, valid_size=0.3, random_state=2018, shuffle=True, is_debug=False):\n",
    "    \"\"\"\n",
    "    训练集文本转ID\n",
    "    :param valid_size: 验证集大小\n",
    "    \"\"\"\n",
    "    print(os.path.join(config.ROOT_DIR, config.TRAIN_FILE))\n",
    "    if len(glob(os.path.join(config.ROOT_DIR, config.TRAIN_FILE))) > 0:\n",
    "        logger.info('Text to id file existed')\n",
    "        return\n",
    "    logger.info('Text to id')\n",
    "    sentences, labels, lengths = [], [], []\n",
    "    size = 0\n",
    "    with open(os.path.join(config.ROOT_DIR, config.RAW_DATA), 'r') as fr:\n",
    "        for line in tqdm(fr, desc='text_to_id'):\n",
    "            words, label = sent_label_split(line)\n",
    "            sent = [word_to_id(word=word, word2id=word2id) for word in words]\n",
    "            if maxlen:\n",
    "                sent = sent[:maxlen]\n",
    "            length = len(sent)\n",
    "            sentences.append(sent)\n",
    "            labels.append(label)\n",
    "            lengths.append(length)\n",
    "            size += 1\n",
    "            if is_debug:\n",
    "                limit_train_size = 10000\n",
    "                if size > limit_train_size:\n",
    "                    break\n",
    "\n",
    "    train, valid = train_val_split(sentences, labels,\n",
    "                                   valid_size=valid_size,\n",
    "                                   random_state=random_state,\n",
    "                                   shuffle=shuffle)\n",
    "    del sentences, labels, lengths\n",
    "\n",
    "\n",
    "    with open(config.TRAIN_FILE, 'w') as fw:\n",
    "        for sent, label in train:\n",
    "            sent = [str(s) for s in sent]\n",
    "            line = \"\\t\".join[str(label), \" \".join(sent)]\n",
    "            fw.write(line + '\\n')\n",
    "        logger.info('Writing train to file done')\n",
    "\n",
    "    with open(config.VALID_FILE, 'w') as fw:\n",
    "        for sent, label in train:\n",
    "            sent = [str(s) for s in sent]\n",
    "            line = \"\\t\".join[str(label), \" \".join(sent)]\n",
    "            fw.write(line + '\\n')\n",
    "        logger.info('Writing valid to file done')\n",
    "\n",
    "\n",
    "# 功能整合，提供给外部调用的函数接口\n",
    "def data_helper(vocab_size, min_freq=3, stop_list=None,\n",
    "                valid_size=0.3, random_state=2018, shuffle=True, is_debug=False):\n",
    "    # 判断文件是否已存在\n",
    "    if len(glob(os.path.join(config.ROOT_DIR, config.WORD2ID_FILE))) > 0:\n",
    "        logger.info('Word to id file existed')\n",
    "        with open(os.path.join(config.ROOT_DIR, config.WORD2ID_FILE), 'rb') as fr:\n",
    "            word2id = pickle.load(fr)\n",
    "    else:\n",
    "        word2id = bulid_vocab(vocab_size=vocab_size, min_freq=min_freq, stop_word_list=stop_list,\n",
    "                is_debug=is_debug)\n",
    "    text2id(word2id, valid_size=valid_size, random_state=random_state, shuffle=shuffle, is_debug=is_debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------PATH------------\n",
    "ROOT_DIR = '/home/daizelin/pytorch/'\n",
    "RAW_DATA = 'data/data_for_test.csv'\n",
    "TRAIN_FILE = 'outputs/intermediate/train.tsv'\n",
    "VALID_FILE = 'outputs/intermediate/valid.tsv'\n",
    "LOG_PATH = 'outputs/logs'\n",
    "is_debug = False\n",
    "flag_words = ['<pad>', '<unk>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logginger.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import Logger\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "\n",
    "'''\n",
    "使用方式\n",
    "from you_logging_filename.py import init_logger\n",
    "logger = init_logger(\"dataset\",logging_path='')\n",
    "def you_function():\n",
    "\tlogger.info()\n",
    "\tlogger.error()\n",
    "\n",
    "'''\n",
    "'''\n",
    "日志模块\n",
    "1. 同时将日志打印到屏幕跟文件中\n",
    "2. 默认值保留近7天日志文件\n",
    "'''\n",
    "def init_logger(logger_name, logging_path):\n",
    "    if logger_name not in Logger.manager.loggerDict:\n",
    "        logger  = logging.getLogger(logger_name)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        handler = TimedRotatingFileHandler(filename=logging_path+\"/all.log\",when='D',backupCount = 7)\n",
    "        datefmt = '%Y-%m-%d %H:%M:%S'\n",
    "        format_str = '[%(asctime)s]: %(name)s %(filename)s[line:%(lineno)s] %(levelname)s  %(message)s'\n",
    "        formatter = logging.Formatter(format_str,datefmt)\n",
    "        handler.setFormatter(formatter)\n",
    "        handler.setLevel(logging.INFO)\n",
    "        logger.addHandler(handler)\n",
    "        console= logging.StreamHandler()\n",
    "        console.setLevel(logging.INFO)\n",
    "        console.setFormatter(formatter)\n",
    "        logger.addHandler(console)\n",
    "\n",
    "        handler = TimedRotatingFileHandler(filename=logging_path+\"/error.log\",when='D',backupCount=7)\n",
    "        datefmt = '%Y-%m-%d %H:%M:%S'\n",
    "        format_str = '[%(asctime)s]: %(name)s %(filename)s[line:%(lineno)s] %(levelname)s  %(message)s'\n",
    "        formatter = logging.Formatter(format_str,datefmt)\n",
    "        handler.setFormatter(formatter)\n",
    "        handler.setLevel(logging.ERROR)\n",
    "        logger.addHandler(handler)\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_CONTEXT_LEN = 512\n",
    "BATCH_SIZE = 128 \n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class IDS():\n",
    "    def __init__(self, fvec):\n",
    "        self.fvec = fvec     \n",
    "        (self.ids, self.vec) = self._ids()      # char : index        index : vector\n",
    "        self.defaultvec = []             \n",
    "        self.defaultvec.extend([0]*128) \n",
    "\n",
    "    def _ids(self):\n",
    "        _IDS = {} \n",
    "        _VEC = {}\n",
    "        for line in open(self.fvec): \n",
    "            _trip_line = line.strip('\\n').strip(' ').split(' ')\n",
    "            if len(_trip_line) == 129:   \n",
    "                _char = _trip_line[0]  \n",
    "                if _char not in _IDS:\n",
    "                    _IDS[_char] = len(_IDS)+1  \n",
    "                    _VEC[_IDS[_char]] = [ float(k) for k in _trip_line[1:]]  \n",
    "        print(\"\\t ... load ids(%s) ...\" %(len(_IDS)))\n",
    "        return _IDS, _VEC       # 单词： index      index ： vector\n",
    "\n",
    "    def get_id(self, char):      # 根据单词获得对应 index\n",
    "        if char in self.ids:\n",
    "            return self.ids[char]\n",
    "        return -1\n",
    "\n",
    "    def get_vector_byid(self, cid):      # 根据index 获取对应词向量\n",
    "        if cid in self.vec:\n",
    "            return self.vec[cid]\n",
    "        return self.defaultvec \n",
    "\n",
    "    def get_vector_bychar(self, char):      # 根据char --> index ----> vector \n",
    "        if char in self.ids:\n",
    "            return self.vec[self.ids[char]]\n",
    "        return self.defaultvec \n",
    "\n",
    "    def get_ids(self, txtlist):\n",
    "        return [self.get_id(c) for c in txtlist]     # 获取文本的index 列表\n",
    "\n",
    "    def get_vectors(self, idlist):\n",
    "        _vecs = []\n",
    "        for i in idlist:\n",
    "            _vecs.append(self.get_vector_byid(i))     # 获取文本index列表对应的vector列表\n",
    "        return _vecs\n",
    "\n",
    "# 这个lr调整的函数，在这里不适用。调整幅度过大\n",
    "def adjust_learning_rate(optimizer, _batch_num):\n",
    "    lr = (1*1e-5) / (_batch_num/2000)     \n",
    "    for param_group in optimizer.param_groups:\n",
    "        \"\"\"\n",
    "        optimizer.param_groups:是长度为2的list，其中元素是2个字典， \n",
    "        optimizer.param_groups[0]:长度为6的字典，包括['amsgrad','params','lr','betas','weight_decay','eps']\n",
    "        \"\"\"\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "class dataOperator():\n",
    "        \"\"\" 类变量，可以直接用类调用，或用实例对象调用\"\"\"\n",
    "    def __init__(self, f_train, f_test, _idobj):\n",
    "        self.file_train = f_train \n",
    "        self.file_test = f_test    # 实例变量\n",
    "        self.idobj = _idobj       # 类对象\n",
    "        (__, self.train_txt, self.train_labels, self.train_mask) = self._load_txt(self.file_train, maxcnt=4000000)    # 训练样本向量\n",
    "        (__, self.test_txt, self.test_labels, self.test_mask) = self._load_txt(self.file_test, maxcnt=50000)\n",
    "        self.epoch_num = int(len(self.train_txt)/BATCH_SIZE) \n",
    "  \n",
    "    def _load_txt(self, fname, maxcnt=100000):\n",
    "        _samples = []\n",
    "        _labels = []\n",
    "        _masks = []   #局部变量\n",
    "        if not os.path.isfile(fname):   # 处理文件是否存在\n",
    "            return (False, _samples, _labels)\n",
    "        print(\"\\t ... load speakers (%s) ...\" %(fname))\n",
    "        cnt = 0\n",
    "        for line in open(fname): \n",
    "            cnt += 1\n",
    "            if cnt % 100000 == 0: \n",
    "                print(\"\\t ... load samples(%s) ...\" %(cnt))   # 加载样本的数量\n",
    "            _trip_line = line.strip('\\n').strip(' ').split(' ')             \n",
    "            if \"_label_\" in _trip_line[0] and len(_trip_line) < MAX_CONTEXT_LEN - 1:\n",
    "                _lab = _trip_line[0] \n",
    "                _t_ids = self.idobj.get_ids(_trip_line[1:])            # 文本对应的index列表\n",
    "                _t_ids.extend([0]*(MAX_CONTEXT_LEN - len(_t_ids)))     # 长度不足512的用0补充\n",
    "                _samples.append(self.idobj.get_vectors(_t_ids))        # 样本向量list\n",
    "                _labels.append(int(_lab.replace('__label__', \"\")))\n",
    "                # 计算mask, False表示不mask\n",
    "                _masks.append([ False if k!=0 else True for k in _t_ids ])  #mask  长足不足0补充的位置为 True，相应网络参数不更新\n",
    "            if cnt > maxcnt:     #样本量 \n",
    "                break\n",
    "        return (True, _samples, _labels, _masks)\n",
    "\n",
    "    def shuffle_train_data(self):\n",
    "        _index = [i for i in range(len(self.train_txt))]\n",
    "        random.shuffle(_index)\n",
    "        self.epoch_train_indexs = _index  #调用方法的时候才会有是这个实例变量，init是生成一个类都会有的公共实例变量\n",
    "        self.batch_cnt_train = 0\n",
    "        print(\"\\t ... shuffle_train_data done ...\")\n",
    "\n",
    "    def shuffle_test_data(self):\n",
    "        _index = [i for i in range(len(self.test_txt))]\n",
    "        random.shuffle(_index)\n",
    "        self.epoch_test_indexs = _index\n",
    "        self.batch_cnt_test = 0\n",
    "        print(\"\\t ... shuffle_test_data done ...\")\n",
    "\n",
    "    def get_batch_in_trainset(self): \n",
    "        _ret = self.epoch_train_indexs[self.batch_cnt_train*BATCH_SIZE: (self.batch_cnt_train+1)*BATCH_SIZE]       #单个epoch 一个batch的数据集合   \n",
    "        _t_data = [] \n",
    "        _t_labels = []\n",
    "        _t_mask = []\n",
    "        for _index in _ret:\n",
    "            _t_data.append(self.train_txt[_index]) \n",
    "            _t_labels.append(self.train_labels[_index]) \n",
    "            _t_mask.append(self.train_mask[_index]) \n",
    "        # set batch_cnt\n",
    "        self.batch_cnt_train += 1\n",
    "        if self.batch_cnt_train > self.epoch_num - 2:   #最后一个batch_size\n",
    "            self.batch_cnt_train = 0\n",
    "        \"\"\"\n",
    "        torch.Tensor转list   :list =tensor.numpy().tolist()\n",
    "        torch.Tensor 转numpy: ndarry=tensor.cpu().numpy()  gpu上不能直接转\n",
    "        numpy 转torch.tensor:  tensor= torch.from_numpy(ndarry)\n",
    "        \"\"\"\n",
    "\n",
    "        return torch.FloatTensor(_t_data), torch.LongTensor(_t_labels), torch.BoolTensor(_t_mask)   #转化为tensor      \n",
    "\n",
    "    def get_batch_in_testset(self): \n",
    "        _ret = self.epoch_test_indexs[self.batch_cnt_test*BATCH_SIZE: (self.batch_cnt_test+1)*BATCH_SIZE]\n",
    "        _t_data = []\n",
    "        _t_labels = []\n",
    "        _t_mask = []\n",
    "        for _index in _ret:\n",
    "            _t_data.append(self.test_txt[_index]) \n",
    "            _t_labels.append(self.test_labels[_index]) \n",
    "            _t_mask.append(self.test_mask[_index]) \n",
    "        # set batch_cnt\n",
    "        self.batch_cnt_test += 1\n",
    "        if self.batch_cnt_test > int(len(self.test_txt)/BATCH_SIZE) - 2:\n",
    "            self.batch_cnt_test = 0\n",
    "        return torch.FloatTensor(_t_data), torch.LongTensor(_t_labels), torch.BoolTensor(_t_mask)\n",
    "\n",
    "    def set_train_dataset(self):\n",
    "        print(\"\\t ... set train dataset ...\")\n",
    "        print(torch.FloatTensor(self.train_txt).shape)    # 训练样本集格式\n",
    "        print(torch.FloatTensor(self.train_labels).shape) # 标签大小\n",
    "        print(torch.FloatTensor(self.train_mask).shape)   # 样本集mask大小\n",
    "        # set DataLoader\n",
    "        self.train_set = TensorDataset( torch.FloatTensor(self.train_txt),\n",
    "                                        torch.LongTensor(self.train_labels), torch.BoolTensor(self.train_mask) )\n",
    "        self.train_loader = DataLoader(dataset=self.train_set,\n",
    "                              batch_size=int(BATCH_SIZE),\n",
    "                              shuffle=True)\n",
    "\n",
    "        # just for memory ?\n",
    "        #del self.train_txt, self.train_labels, self.train_mask\n",
    "\n",
    "    def set_test_dataset(self):\n",
    "        print(\"\\t ... set test dataset ...\")\n",
    "        \"\"\"\n",
    "        torch 的数据加载到模型的操作顺序是这样的：\n",
    "        1. 创建一个TensorDataset 对象\n",
    "        2. 创建一个DataLoader对象\n",
    "        3. 循环这个DataLoader对象，将train、labels 加载到模型进行训练\n",
    "            \n",
    "        TensorDataset可以用来对tensor进行打包，通过每一个tensor的第一维度进行索引，因此第一维必须相同类似为dataframe，将其他tensor横向拼接，一一对应\n",
    "        \"\"\"\n",
    "        self.test_set = TensorDataset( torch.FloatTensor(self.test_txt), torch.LongTensor(self.test_labels),\n",
    "                                       torch.BoolTensor(self.test_mask) )\n",
    "        print(self.test_set[:2])    #取出各列的前两行\n",
    "        self.test_loader = DataLoader(dataset=self.test_set,        # 传入的数据集\n",
    "                              batch_size=BATCH_SIZE,                #每个batch有多少个样本\n",
    "                              shuffle=True)                         # 每个epoch 开始的时候，对数据进行重新排序\n",
    "                            #num_workers   :data loading 的线程数\n",
    "                            # drop_last   对最后不足batch_size 的数据扔掉还是继续正常执行\n",
    "\n",
    "        # just for memory ?\n",
    "        del self.test_txt, self.test_labels, self.test_mask\n",
    "\n",
    "    def get_test_1batch(self):\n",
    "        _di = iter(self.test_loader)          # 构造test迭代器，每次验证一个batch进行测试\n",
    "        (_data, _target, _mask) = _di.next() \n",
    "        return (_data, _target, _mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
