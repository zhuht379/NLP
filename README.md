# Text_classification_Pytorch


参考资料：

1.https://github.com/649453932/Chinese-Text-Classification-Pytorch

2.https://state-of-art.top/2018/11/28/torchtext%E8%AF%BB%E5%8F%96%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E9%9B%86/

3.https://github.com/graykode/nlp-tutorial

4.https://github.com/jeffery0628/text_classification        
使用rnn,lstm,gru,fasttext,textcnn,dpcnn,rnn-att,lstm-att,bert,bert-cnn,bert-rnn,bert-rcnn,han,xlnet等等做文本分类，以及对比

分词：

1.https://github.com/fastnlp/fastHan/

词向量：

1.https://github.com/zlsdu/Word-Embedding

FastText:

1.https://fasttext.cc/docs/en/supervised-tutorial.html

2.https://blog.csdn.net/feilong_csdn/article/details/88655927

3.https://github.com/facebookresearch/fastText

实体识别：

1.https://mp.weixin.qq.com/s/5-B4IRQ3Y_sTdryu4dNuNQ              https://github.com/LeeSureman/Flat-Lattice-Transformer

情感分析：

1.https://pypi.org/project/cnsenti/

NLP中的对抗训练：

1.https://zhuanlan.zhihu.com/p/91269728

2.https://mp.weixin.qq.com/s/bGtNWqbVnH8fNVIuOiR_Gw

文本摘要生成：

1. https://github.com/zingp/NLP/tree/master/P007PytorchPointerGeneratorNetwork     对应的优化算法：BeamSearch优化思路

2. https://www.cnblogs.com/zingp/p/11571593.html



BiLstm-crf原理和代码实现：

https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html#bi-lstm-conditional-random-field-discussion

1.https://zhuanlan.zhihu.com/p/162885285

2.https://zhuanlan.zhihu.com/p/97829287

3.https://zhuanlan.zhihu.com/p/97858739

BiLSTM-CNN-CRF
参考资料：
https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf

Transformer-CRF
参考资料：
https://github.com/phychaos/transformer_crf

BERT-BiLSTM-CRF
参考资料：
https://github.com/macanv/BERT-BiLSTM-CRF-NER

Transformer
参考资料：
1.https://mp.weixin.qq.com/s/g6EliR8W1AgpLm8QCcxncw

2.https://blog.csdn.net/BigDataDigest/article/details/86088814?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param

3.https://github.com/kimiyoung/transformer-xl/tree/master/pytorc

Bert:
参考资料：
1.https://github.com/xmxoxo/BERT-train2deploy

关于Transformer和BERT的面试题：

1.https://mp.weixin.qq.com/s/S3j0erS_5fAE9bJ7qeutOA

2.https://mp.weixin.qq.com/s/tyuirRUpFeW9bcNI96DvHA

3.各种注意力机制：encoder-decoder,self-attention,multi-head attention的区别

4.https://www.isclab.org.cn/wp-content/uploads/2018/12/Transformer%E4%B8%AD%E7%9A%84Multi-Head-Attention-%E7%8E%8B%E7%9D%BF%E6%80%A1-2018.12.9-19_00_00.pdf

经验：
1.Word Embedding 如何处理未登录词：https://www.zhihu.com/question/308543084
